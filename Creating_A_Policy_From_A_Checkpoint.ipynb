{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for loading a trained policy and turning it into a static agent.\n",
    "\n",
    "Note, this is a bit of a WIP, and only works with torch currently. If you want to implement advisieral or leauge learning, this might be a good starting point, but not a full implmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "  import distutils.spawn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from poker_env import PokerEnv\n",
    "from agents.random_policy import RandomActions\n",
    "from agents.heuristic_policy import HeuristicPolicy\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from gym import spaces\n",
    "import mpu\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.models import MODEL_DEFAULTS\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the config that trained the policy you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_policy(agent_id, episode, **kwargs):\n",
    "    if agent_id == 0:\n",
    "        return \"learned\"\n",
    "    elif agent_id == 1:\n",
    "        return \"Heuristic_10\"\n",
    "    elif agent_id == 2:\n",
    "        return \"Heuristic_100\"\n",
    "    elif agent_id == 3:\n",
    "        return \"Heuristic_1000\"\n",
    "    return \"Heuristic_1000\"\n",
    "\n",
    "def env_creator(config):\n",
    "    env = PokerEnv(select_policy, config)\n",
    "    return env\n",
    "\n",
    "register_env(\"poker\", lambda config: env_creator(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:15:56,095\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=35091)\u001b[0m /home/ubuntu/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "\u001b[2m\u001b[36m(pid=35091)\u001b[0m   import distutils.spawn\n",
      "\u001b[2m\u001b[36m(pid=35093)\u001b[0m /home/ubuntu/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "\u001b[2m\u001b[36m(pid=35093)\u001b[0m   import distutils.spawn\n",
      "2022-11-10 14:16:05,776\tINFO trainable.py:164 -- Trainable.setup took 11.560 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-11-10 14:16:05,778\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# This should be the same as config used to train a model? \n",
    "\n",
    "heuristic_observation_space = spaces.Dict({\n",
    "            \"hand\": spaces.Box(0, 1, shape=(24, )),\n",
    "            \"community\": spaces.Box(0, 1, shape=(24, ))\n",
    "        })\n",
    "action_space = spaces.Discrete(3)\n",
    "\n",
    "#Defines the learning models architecture. \n",
    "model = MODEL_DEFAULTS.update({'fcnet_hiddens': [512, 512], 'fcnet_activation': 'relu'})\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    #Each rollout worker uses a single cpu\n",
    "    .rollouts(num_rollout_workers=2, num_envs_per_worker=1)\\\n",
    "    .training(train_batch_size=4000, gamma=0.99, model=model, lr=0.0004)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            #These policies thave pre-definded polices that dont learn.\n",
    "            \"random\": PolicySpec(policy_class=RandomActions),\n",
    "            \"Heuristic_10\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 0}),\n",
    "            \"Heuristic_100\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 1}),\n",
    "            \"Heuristic_1000\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 2}),\n",
    "            #Passing nothing causes this agent to deafult to using a PPO policy\n",
    "            \"learned\": PolicySpec(\n",
    "                config={}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn=select_policy,\n",
    "        policies_to_train=['learned'],\n",
    "    )\\\n",
    "    .resources(num_gpus=0)\\\n",
    "    .framework('torch')\n",
    ")\n",
    "trainer = config.build(env=\"poker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:16:05,819\tINFO trainable.py:766 -- Restored on 172.31.11.130 from checkpoint: checkpoint/ppo_poker/checkpoint_000010\n",
      "2022-11-10 14:16:05,820\tINFO trainable.py:775 -- Current state after restoring: {'_iteration': 10, '_timesteps_total': None, '_time_total': 69.48709058761597, '_episodes_total': 4117}\n"
     ]
    }
   ],
   "source": [
    "# Restore from the checkpoint\n",
    "trainer.restore('checkpoint/ppo_poker/checkpoint_000010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get out the trained policy\n",
    "trainer.get_policy('learned').export_model('models/ppo_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load('models/ppo_agent/model.pt')\n",
    "#checkpoint/ppo_poker/checkpoint_000010\n",
    "model.eval()\n",
    "dic = {'obs': torch.tensor(np.zeros((69,1)).reshape(1, -1))}\n",
    "torch.argmax(model(dic, [torch.tensor(np.zeros(0))], torch.tensor(np.zeros(1)))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up the training env using the pre trained agent as a static agent in the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "from ray.rllib.utils.spaces.space_utils import (\n",
    "    get_base_struct_from_space)\n",
    "from gym import spaces\n",
    "\n",
    "class TrainedPolicyAgent(Policy):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  \n",
    "        #model = tf.saved_model.load('models/ppo_agent')\n",
    "        #self.inference = model.signatures[\"serving_default\"]\n",
    "        self.model = torch.load('models/ppo_agent/model.pt')\n",
    "        self.model.eval()\n",
    "        \n",
    "    def get_initial_state(self):\n",
    "        return [0]\n",
    "        \n",
    "    def compute_actions(\n",
    "            self,\n",
    "            obs_batch,\n",
    "            state_batches=None,\n",
    "            prev_action_batch=None,\n",
    "            prev_reward_batch=None,\n",
    "            info_batch=None,\n",
    "            episodes=None,\n",
    "            **kwargs\n",
    "        ):\n",
    "        #return inference(is_training=tf.constant(False), observations=obs_batch, timestep=tf.constant(-1, dtype=tf.int64))['actions_0'][0]\n",
    "        dic = {'obs': torch.tensor(obs_batch.reshape(1, -1))}\n",
    "        return [torch.argmax(self.model(dic, [torch.tensor(np.zeros(0))], torch.tensor(np.zeros(1)))[0]).item()], [], {}\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=35221)\u001b[0m /home/ubuntu/.local/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:9: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives\n",
      "\u001b[2m\u001b[36m(pid=35221)\u001b[0m   import distutils.spawn\n",
      "2022-11-10 14:16:13,109\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def select_policy_new_agent(agent_id, episode, **kwargs):\n",
    "    if agent_id == 0:\n",
    "        return \"learned\"\n",
    "    elif agent_id == 1:\n",
    "        return \"Heuristic_10\"\n",
    "    elif agent_id == 2:\n",
    "        return \"Heuristic_100\"\n",
    "    elif agent_id == 3:\n",
    "        return \"PPO_Agent\"\n",
    "    return \"Heuristic_1000\"\n",
    "\n",
    "# TODO this overrides stuff above....  \n",
    "def env_creator(config):\n",
    "    env = PokerEnv(select_policy_new_agent, config)\n",
    "    return env\n",
    "\n",
    "    \n",
    "register_env(\"poker\", lambda config: env_creator(config))\n",
    "\n",
    "PPO_Agent_observation_space = spaces.Dict({\n",
    "            \"obs\": spaces.Box(0, 400, shape=(24+24+16+4, )),\n",
    "            \"state\": spaces.Box(0, 1, shape=(1, )),\n",
    "        })\n",
    "\n",
    "heuristic_observation_space = spaces.Dict({\n",
    "            \"hand\": spaces.Box(0, 1, shape=(24, )),\n",
    "            \"community\": spaces.Box(0, 1, shape=(24, ))\n",
    "        })\n",
    "\n",
    "action_space = spaces.Discrete(3)\n",
    "\n",
    "model = MODEL_DEFAULTS.update({'fcnet_hiddens': [512, 512], 'fcnet_activation': 'relu'})\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rollouts(num_rollout_workers=1, num_envs_per_worker=1)\\\n",
    "    .training(train_batch_size=4000, gamma=0.99, model=model, lr=0.0004)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"random\": PolicySpec(policy_class=RandomActions),\n",
    "            \"Heuristic_10\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 0}),\n",
    "            \"Heuristic_100\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 1}),\n",
    "            \"PPO_Agent\": (TrainedPolicyAgent, PPO_Agent_observation_space, action_space, {}),\n",
    "            \"learned\": PolicySpec(\n",
    "                config={}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn=select_policy_new_agent,\n",
    "        policies_to_train=['learned'],\n",
    "    )\\\n",
    "    .resources(num_gpus=0)\\\n",
    "    .framework('torch')\n",
    ")\n",
    "trainer = config.build(env=\"poker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:16:19,118\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:18:07,272\tWARNING policy.py:121 -- Can not figure out a durable policy name for <class 'agents.heuristic_policy.HeuristicPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n",
      "2022-11-10 14:18:07,276\tWARNING policy.py:121 -- Can not figure out a durable policy name for <class '__main__.TrainedPolicyAgent'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n",
      "2022-11-10 14:18:07,278\tWARNING policy.py:121 -- Can not figure out a durable policy name for <class 'agents.heuristic_policy.HeuristicPolicy'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n",
      "2022-11-10 14:18:07,280\tWARNING policy.py:121 -- Can not figure out a durable policy name for <class 'agents.random_policy.RandomActions'>. You are probably trying to checkpoint a custom policy. Raw policy class may cause problems when the checkpoint needs to be loaded in the future. To fix this, make sure you add your custom policy in rllib.algorithms.registry.POLICIES.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'checkpoint/ppo_poker_checkpoint_test/checkpoint_000010'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.save(\"checkpoint/ppo_poker_checkpoint_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "03af9a79c2dca760cc1a7eb3ed3e88bcac6dcfea843cc60b832e817055251c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
