{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "/Users/adamprice/Desktop/Poker/Notebooks/../poker_env_multi.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from poker_env_multi import PokerEnvMulti\n",
    "from agents.random_policy import RandomActions\n",
    "from agents.heuristic_policy import HeuristicPolicy\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from gym import spaces\n",
    "import mpu\n",
    "import numpy as np\n",
    "import ray\n",
    "\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def select_policy(agent_id, episode, **kwargs):\n",
    "    if agent_id == 0:\n",
    "        return \"learned\"\n",
    "    elif agent_id == 1:\n",
    "        return \"Heuristic_10\"\n",
    "    elif agent_id == 2:\n",
    "        return \"Heuristic_100\"\n",
    "    elif agent_id == 3:\n",
    "        return \"Heuristic_1000\"\n",
    "    return \"Heuristic_1000\"\n",
    "\n",
    "def env_creator(config):\n",
    "    env = PokerEnvMulti(select_policy, config)\n",
    "    return env\n",
    "\n",
    "register_env(\"poker\", lambda config: env_creator(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 17:39:02,342\tWARNING ppo.py:350 -- `train_batch_size` (128) cannot be achieved with your other settings (num_workers=0 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 128.\n",
      "/Users/adamprice/Applications/anaconda3/envs/stocktake/lib/python3.8/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.1/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.\n",
      "  warnings.warn(\n",
      "2022-11-07 17:39:05,799\tINFO worker.py:1518 -- Started a local Ray instance.\n",
      "2022-11-07 17:39:08,151\tWARNING env.py:235 -- Your MultiAgentEnv <PokerEnvMulti instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "2022-11-07 17:39:13,829\tINFO trainable.py:162 -- Trainable.setup took 11.493 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-11-07 17:39:13,831\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "heuristic_observation_space = spaces.Dict({\n",
    "            \"hand\": spaces.Box(0, 1, shape=(28, )),\n",
    "            \"community\": spaces.Box(0, 1, shape=(28, ))\n",
    "        })\n",
    "action_space = spaces.Discrete(3)\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .rollouts(\n",
    "        num_rollout_workers=0,\n",
    "        num_envs_per_worker=1,\n",
    "    )\n",
    "    .training(\n",
    "        train_batch_size=128,\n",
    "        gamma=0.99,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            \"random\": PolicySpec(policy_class=RandomActions),\n",
    "            \"Heuristic_10\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 0}),\n",
    "            \"Heuristic_100\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 1}),\n",
    "            \"Heuristic_1000\": (HeuristicPolicy, heuristic_observation_space, action_space, {'difficulty': 2}),\n",
    "            \"learned\": PolicySpec(\n",
    "                config={}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn=select_policy,\n",
    "        policies_to_train=['learned'],\n",
    "    )\n",
    "    .resources(num_gpus=0)\\\n",
    ")\n",
    "trainer = config.build(env=\"poker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['KING of DIAMONDS; TWO of SPADES; ', 'KING of CLUBS; THREE of DIAMONDS; ', 'KING of HEARTS; THREE of CLUBS; ', 'TWO of HEARTS; THREE of HEARTS; '], 'community': 'ACE of CLUBS; TWO of CLUBS; QUEEN of SPADES; TWO of DIAMONDS; JACK of HEARTS; ', 'scores': array([[3., 1., 6.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [3., 1., 5.]]), 'winners': 0, 'chips': [65, 138, 97, 98], 'bets': array([[2., 1., 1., 1.],\n",
      "       [2., 1., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [2., 1., 1., 1.]])}\n",
      "[  0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  65. 137.  95.  96.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['JACK of DIAMONDS; TWO of CLUBS; ', 'THREE of DIAMONDS; FOUR of CLUBS; ', 'THREE of HEARTS; JACK of HEARTS; ', 'JACK of CLUBS; ACE of CLUBS; '], 'community': 'JACK of SPADES; TWO of HEARTS; KING of SPADES; FOUR of DIAMONDS; TWO of DIAMONDS; ', 'scores': array([[0., 0., 0.],\n",
      "       [2., 3., 6.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.]]), 'winners': 1, 'chips': [65, 144, 93, 96], 'bets': array([[0., 0., 0., 0.],\n",
      "       [2., 1., 1., 2.],\n",
      "       [2., 1., 1., 0.],\n",
      "       [2., 0., 0., 0.]])}\n",
      "[  0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  65. 144.  92.  94.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['FOUR of CLUBS; FOUR of HEARTS; ', 'QUEEN of HEARTS; JACK of SPADES; ', 'ACE of SPADES; QUEEN of CLUBS; ', 'FOUR of DIAMONDS; TWO of SPADES; '], 'community': 'ACE of DIAMONDS; TWO of CLUBS; ACE of CLUBS; THREE of SPADES; QUEEN of DIAMONDS; ', 'scores': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [6., 0., 5.],\n",
      "       [2., 1., 5.]]), 'winners': 2, 'chips': [65, 142, 101, 90], 'bets': array([[0., 0., 0., 0.],\n",
      "       [1., 1., 0., 0.],\n",
      "       [1., 1., 2., 2.],\n",
      "       [1., 1., 2., 2.]])}\n",
      "[  0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  63. 141.  99.  89.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['TWO of SPADES; ACE of DIAMONDS; ', 'TWO of HEARTS; JACK of SPADES; ', 'KING of HEARTS; FOUR of DIAMONDS; ', 'THREE of HEARTS; QUEEN of DIAMONDS; '], 'community': 'KING of CLUBS; FOUR of SPADES; TWO of DIAMONDS; ACE of CLUBS; JACK of HEARTS; ', 'scores': array([[0., 0., 0.],\n",
      "       [2., 4., 6.],\n",
      "       [2., 6., 6.],\n",
      "       [0., 0., 0.]]), 'winners': 2, 'chips': [63, 139, 107, 89], 'bets': array([[2., 0., 0., 0.],\n",
      "       [1., 0., 1., 1.],\n",
      "       [1., 0., 1., 1.],\n",
      "       [1., 0., 0., 0.]])}\n",
      "[  1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  62. 137. 106.  88.]\n",
      "progress\n",
      "[  1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   1.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   1.   0.   0.   0.   1.   0.   0.   0.   1.   0.   0.   0.   1.   0.\n",
      "   0.   0.  62. 138. 105.  87.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['JACK of HEARTS; ACE of CLUBS; ', 'ACE of DIAMONDS; FOUR of HEARTS; ', 'QUEEN of DIAMONDS; THREE of CLUBS; ', 'THREE of HEARTS; ACE of SPADES; '], 'community': 'THREE of SPADES; JACK of SPADES; FOUR of CLUBS; KING of DIAMONDS; ACE of HEARTS; ', 'scores': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [4., 6., 6.],\n",
      "       [2., 2., 6.]]), 'winners': 2, 'chips': [62, 135, 116, 85], 'bets': array([[1., 0., 0., 0.],\n",
      "       [1., 1., 2., 0.],\n",
      "       [1., 1., 2., 0.],\n",
      "       [1., 1., 2., 0.]])}\n",
      "[  0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  62. 134. 114.  84.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "done\n",
      "{'hands': ['ACE of HEARTS; JACK of CLUBS; ', 'TWO of SPADES; THREE of DIAMONDS; ', 'FOUR of CLUBS; QUEEN of HEARTS; ', 'ACE of DIAMONDS; FOUR of HEARTS; '], 'community': 'QUEEN of CLUBS; ACE of SPADES; JACK of DIAMONDS; TWO of DIAMONDS; KING of HEARTS; ', 'scores': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [1., 0., 6.]]), 'winners': [], 'chips': [62, 134, 114, 88], 'bets': array([[0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 1., 0., 0.],\n",
      "       [1., 1., 0., 0.]])}\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  62. 134. 113.  86.]\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  61. 133. 112.  86.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['FOUR of DIAMONDS; TWO of DIAMONDS; ', 'JACK of CLUBS; KING of DIAMONDS; ', 'JACK of SPADES; FOUR of CLUBS; ', 'FOUR of HEARTS; THREE of CLUBS; '], 'community': 'ACE of SPADES; ACE of HEARTS; ACE of DIAMONDS; TWO of CLUBS; ACE of CLUBS; ', 'scores': array([[0., 0., 0.],\n",
      "       [7., 0., 6.],\n",
      "       [7., 0., 4.],\n",
      "       [7., 0., 3.]]), 'winners': 1, 'chips': [61, 143, 110, 84], 'bets': array([[1., 0., 0., 0.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]])}\n",
      "[  0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  59. 142. 109.  82.]\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "progress\n",
      "showdown\n",
      "done\n",
      "{'hands': ['JACK of HEARTS; THREE of CLUBS; ', 'TWO of HEARTS; JACK of SPADES; ', 'QUEEN of DIAMONDS; FOUR of CLUBS; ', 'JACK of CLUBS; FOUR of HEARTS; '], 'community': 'QUEEN of HEARTS; ACE of CLUBS; THREE of SPADES; FOUR of SPADES; KING of CLUBS; ', 'scores': array([[0., 0., 0.],\n",
      "       [0., 0., 0.],\n",
      "       [2., 5., 6.],\n",
      "       [4., 6., 6.]]), 'winners': 3, 'chips': [59, 142, 106, 91], 'bets': array([[2., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1.]])}\n",
      "[  0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.  58. 140. 105.  89.]\n",
      "progress\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'custom_metrics': {},\n",
       " 'episode_media': {},\n",
       " 'num_recreated_workers': 0,\n",
       " 'info': {'learner': {'learned': {'learner_stats': {'cur_kl_coeff': 0.10000000149011612,\n",
       "     'cur_lr': 4.999999873689376e-05,\n",
       "     'total_loss': 1.7376896,\n",
       "     'policy_loss': 0.04304852,\n",
       "     'vf_loss': 1.6927345,\n",
       "     'vf_explained_var': -0.0011304895,\n",
       "     'kl': 0.01906628,\n",
       "     'entropy': 1.054242,\n",
       "     'entropy_coeff': 0.0,\n",
       "     'model': {}}}},\n",
       "  'num_env_steps_sampled': 640,\n",
       "  'num_env_steps_trained': 640,\n",
       "  'num_agent_steps_sampled': 637,\n",
       "  'num_agent_steps_trained': 637},\n",
       " 'sampler_results': {'episode_reward_max': 11.0,\n",
       "  'episode_reward_min': -5.0,\n",
       "  'episode_reward_mean': 2.1025641025641026,\n",
       "  'episode_len_mean': 16.256410256410255,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 8,\n",
       "  'policy_reward_min': {'learned': -8.0,\n",
       "   'Heuristic_10': -8.0,\n",
       "   'Heuristic_100': -9.0,\n",
       "   'Heuristic_1000': -10.0},\n",
       "  'policy_reward_max': {'learned': 13.0,\n",
       "   'Heuristic_10': 18.0,\n",
       "   'Heuristic_100': 16.0,\n",
       "   'Heuristic_1000': 16.0},\n",
       "  'policy_reward_mean': {'learned': -1.3076923076923077,\n",
       "   'Heuristic_10': 1.8717948717948718,\n",
       "   'Heuristic_100': 1.1025641025641026,\n",
       "   'Heuristic_1000': 0.4358974358974359},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [1.0,\n",
       "    3.0,\n",
       "    -5.0,\n",
       "    4.0,\n",
       "    5.0,\n",
       "    -2.0,\n",
       "    7.0,\n",
       "    6.0,\n",
       "    3.0,\n",
       "    5.0,\n",
       "    2.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    3.0,\n",
       "    -5.0,\n",
       "    4.0,\n",
       "    1.0,\n",
       "    1.0,\n",
       "    -1.0,\n",
       "    5.0,\n",
       "    2.0,\n",
       "    -2.0,\n",
       "    -1.0,\n",
       "    2.0,\n",
       "    3.0,\n",
       "    1.0,\n",
       "    4.0,\n",
       "    11.0,\n",
       "    2.0,\n",
       "    4.0,\n",
       "    2.0,\n",
       "    0.0,\n",
       "    4.0,\n",
       "    2.0,\n",
       "    -1.0,\n",
       "    2.0,\n",
       "    4.0,\n",
       "    3.0],\n",
       "   'episode_lengths': [17,\n",
       "    9,\n",
       "    16,\n",
       "    19,\n",
       "    13,\n",
       "    12,\n",
       "    17,\n",
       "    10,\n",
       "    18,\n",
       "    15,\n",
       "    17,\n",
       "    15,\n",
       "    29,\n",
       "    18,\n",
       "    6,\n",
       "    26,\n",
       "    25,\n",
       "    17,\n",
       "    21,\n",
       "    20,\n",
       "    11,\n",
       "    14,\n",
       "    23,\n",
       "    22,\n",
       "    15,\n",
       "    7,\n",
       "    11,\n",
       "    14,\n",
       "    14,\n",
       "    10,\n",
       "    17,\n",
       "    15,\n",
       "    13,\n",
       "    17,\n",
       "    14,\n",
       "    14,\n",
       "    13,\n",
       "    30,\n",
       "    20],\n",
       "   'policy_learned_reward': [0.0,\n",
       "    0.0,\n",
       "    -6.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -2.0,\n",
       "    -1.0,\n",
       "    7.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -3.0,\n",
       "    -1.0,\n",
       "    -6.0,\n",
       "    -1.0,\n",
       "    -1.0,\n",
       "    -8.0,\n",
       "    -2.0,\n",
       "    0.0,\n",
       "    -7.0,\n",
       "    -8.0,\n",
       "    -1.0,\n",
       "    0.0,\n",
       "    -6.0,\n",
       "    -1.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -6.0,\n",
       "    -1.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -3.0,\n",
       "    13.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -2.0,\n",
       "    -2.0,\n",
       "    0.0,\n",
       "    -1.0,\n",
       "    -2.0],\n",
       "   'policy_Heuristic_10_reward': [8.0,\n",
       "    -2.0,\n",
       "    -4.0,\n",
       "    -1.0,\n",
       "    -1.0,\n",
       "    -4.0,\n",
       "    9.0,\n",
       "    -1.0,\n",
       "    12.0,\n",
       "    9.0,\n",
       "    9.0,\n",
       "    15.0,\n",
       "    -4.0,\n",
       "    -7.0,\n",
       "    0.0,\n",
       "    18.0,\n",
       "    6.0,\n",
       "    -1.0,\n",
       "    12.0,\n",
       "    14.0,\n",
       "    0.0,\n",
       "    -3.0,\n",
       "    -4.0,\n",
       "    -2.0,\n",
       "    -8.0,\n",
       "    0.0,\n",
       "    -4.0,\n",
       "    9.0,\n",
       "    -1.0,\n",
       "    -1.0,\n",
       "    -1.0,\n",
       "    -6.0,\n",
       "    12.0,\n",
       "    -4.0,\n",
       "    -3.0,\n",
       "    -8.0,\n",
       "    -1.0,\n",
       "    13.0,\n",
       "    -2.0],\n",
       "   'policy_Heuristic_100_reward': [-5.0,\n",
       "    -4.0,\n",
       "    15.0,\n",
       "    7.0,\n",
       "    -1.0,\n",
       "    -3.0,\n",
       "    -1.0,\n",
       "    0.0,\n",
       "    -5.0,\n",
       "    -2.0,\n",
       "    -2.0,\n",
       "    -8.0,\n",
       "    16.0,\n",
       "    11.0,\n",
       "    4.0,\n",
       "    -9.0,\n",
       "    -6.0,\n",
       "    8.0,\n",
       "    0.0,\n",
       "    -7.0,\n",
       "    7.0,\n",
       "    7.0,\n",
       "    -3.0,\n",
       "    -4.0,\n",
       "    -6.0,\n",
       "    -3.0,\n",
       "    0.0,\n",
       "    -4.0,\n",
       "    6.0,\n",
       "    4.0,\n",
       "    13.0,\n",
       "    0.0,\n",
       "    -8.0,\n",
       "    14.0,\n",
       "    9.0,\n",
       "    13.0,\n",
       "    -2.0,\n",
       "    -4.0,\n",
       "    -4.0],\n",
       "   'policy_Heuristic_1000_reward': [-2.0,\n",
       "    9.0,\n",
       "    -10.0,\n",
       "    -2.0,\n",
       "    7.0,\n",
       "    7.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    -4.0,\n",
       "    -2.0,\n",
       "    -2.0,\n",
       "    -5.0,\n",
       "    -5.0,\n",
       "    -2.0,\n",
       "    0.0,\n",
       "    -6.0,\n",
       "    6.0,\n",
       "    -6.0,\n",
       "    -4.0,\n",
       "    0.0,\n",
       "    -1.0,\n",
       "    -2.0,\n",
       "    11.0,\n",
       "    6.0,\n",
       "    16.0,\n",
       "    6.0,\n",
       "    11.0,\n",
       "    0.0,\n",
       "    6.0,\n",
       "    -1.0,\n",
       "    -5.0,\n",
       "    -5.0,\n",
       "    -4.0,\n",
       "    -6.0,\n",
       "    -2.0,\n",
       "    -4.0,\n",
       "    5.0,\n",
       "    -4.0,\n",
       "    11.0]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.7014085161614947,\n",
       "   'mean_inference_ms': 0.5443387050658153,\n",
       "   'mean_action_processing_ms': 0.09098077690653679,\n",
       "   'mean_env_wait_ms': 0.450588418440034,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0},\n",
       " 'episode_reward_max': 11.0,\n",
       " 'episode_reward_min': -5.0,\n",
       " 'episode_reward_mean': 2.1025641025641026,\n",
       " 'episode_len_mean': 16.256410256410255,\n",
       " 'episodes_this_iter': 8,\n",
       " 'policy_reward_min': {'learned': -8.0,\n",
       "  'Heuristic_10': -8.0,\n",
       "  'Heuristic_100': -9.0,\n",
       "  'Heuristic_1000': -10.0},\n",
       " 'policy_reward_max': {'learned': 13.0,\n",
       "  'Heuristic_10': 18.0,\n",
       "  'Heuristic_100': 16.0,\n",
       "  'Heuristic_1000': 16.0},\n",
       " 'policy_reward_mean': {'learned': -1.3076923076923077,\n",
       "  'Heuristic_10': 1.8717948717948718,\n",
       "  'Heuristic_100': 1.1025641025641026,\n",
       "  'Heuristic_1000': 0.4358974358974359},\n",
       " 'hist_stats': {'episode_reward': [1.0,\n",
       "   3.0,\n",
       "   -5.0,\n",
       "   4.0,\n",
       "   5.0,\n",
       "   -2.0,\n",
       "   7.0,\n",
       "   6.0,\n",
       "   3.0,\n",
       "   5.0,\n",
       "   2.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   3.0,\n",
       "   -5.0,\n",
       "   4.0,\n",
       "   1.0,\n",
       "   1.0,\n",
       "   -1.0,\n",
       "   5.0,\n",
       "   2.0,\n",
       "   -2.0,\n",
       "   -1.0,\n",
       "   2.0,\n",
       "   3.0,\n",
       "   1.0,\n",
       "   4.0,\n",
       "   11.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   0.0,\n",
       "   4.0,\n",
       "   2.0,\n",
       "   -1.0,\n",
       "   2.0,\n",
       "   4.0,\n",
       "   3.0],\n",
       "  'episode_lengths': [17,\n",
       "   9,\n",
       "   16,\n",
       "   19,\n",
       "   13,\n",
       "   12,\n",
       "   17,\n",
       "   10,\n",
       "   18,\n",
       "   15,\n",
       "   17,\n",
       "   15,\n",
       "   29,\n",
       "   18,\n",
       "   6,\n",
       "   26,\n",
       "   25,\n",
       "   17,\n",
       "   21,\n",
       "   20,\n",
       "   11,\n",
       "   14,\n",
       "   23,\n",
       "   22,\n",
       "   15,\n",
       "   7,\n",
       "   11,\n",
       "   14,\n",
       "   14,\n",
       "   10,\n",
       "   17,\n",
       "   15,\n",
       "   13,\n",
       "   17,\n",
       "   14,\n",
       "   14,\n",
       "   13,\n",
       "   30,\n",
       "   20],\n",
       "  'policy_learned_reward': [0.0,\n",
       "   0.0,\n",
       "   -6.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -2.0,\n",
       "   -1.0,\n",
       "   7.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -3.0,\n",
       "   -1.0,\n",
       "   -6.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -8.0,\n",
       "   -2.0,\n",
       "   0.0,\n",
       "   -7.0,\n",
       "   -8.0,\n",
       "   -1.0,\n",
       "   0.0,\n",
       "   -6.0,\n",
       "   -1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -6.0,\n",
       "   -1.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -3.0,\n",
       "   13.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -2.0,\n",
       "   -2.0,\n",
       "   0.0,\n",
       "   -1.0,\n",
       "   -2.0],\n",
       "  'policy_Heuristic_10_reward': [8.0,\n",
       "   -2.0,\n",
       "   -4.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -4.0,\n",
       "   9.0,\n",
       "   -1.0,\n",
       "   12.0,\n",
       "   9.0,\n",
       "   9.0,\n",
       "   15.0,\n",
       "   -4.0,\n",
       "   -7.0,\n",
       "   0.0,\n",
       "   18.0,\n",
       "   6.0,\n",
       "   -1.0,\n",
       "   12.0,\n",
       "   14.0,\n",
       "   0.0,\n",
       "   -3.0,\n",
       "   -4.0,\n",
       "   -2.0,\n",
       "   -8.0,\n",
       "   0.0,\n",
       "   -4.0,\n",
       "   9.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -6.0,\n",
       "   12.0,\n",
       "   -4.0,\n",
       "   -3.0,\n",
       "   -8.0,\n",
       "   -1.0,\n",
       "   13.0,\n",
       "   -2.0],\n",
       "  'policy_Heuristic_100_reward': [-5.0,\n",
       "   -4.0,\n",
       "   15.0,\n",
       "   7.0,\n",
       "   -1.0,\n",
       "   -3.0,\n",
       "   -1.0,\n",
       "   0.0,\n",
       "   -5.0,\n",
       "   -2.0,\n",
       "   -2.0,\n",
       "   -8.0,\n",
       "   16.0,\n",
       "   11.0,\n",
       "   4.0,\n",
       "   -9.0,\n",
       "   -6.0,\n",
       "   8.0,\n",
       "   0.0,\n",
       "   -7.0,\n",
       "   7.0,\n",
       "   7.0,\n",
       "   -3.0,\n",
       "   -4.0,\n",
       "   -6.0,\n",
       "   -3.0,\n",
       "   0.0,\n",
       "   -4.0,\n",
       "   6.0,\n",
       "   4.0,\n",
       "   13.0,\n",
       "   0.0,\n",
       "   -8.0,\n",
       "   14.0,\n",
       "   9.0,\n",
       "   13.0,\n",
       "   -2.0,\n",
       "   -4.0,\n",
       "   -4.0],\n",
       "  'policy_Heuristic_1000_reward': [-2.0,\n",
       "   9.0,\n",
       "   -10.0,\n",
       "   -2.0,\n",
       "   7.0,\n",
       "   7.0,\n",
       "   0.0,\n",
       "   0.0,\n",
       "   -4.0,\n",
       "   -2.0,\n",
       "   -2.0,\n",
       "   -5.0,\n",
       "   -5.0,\n",
       "   -2.0,\n",
       "   0.0,\n",
       "   -6.0,\n",
       "   6.0,\n",
       "   -6.0,\n",
       "   -4.0,\n",
       "   0.0,\n",
       "   -1.0,\n",
       "   -2.0,\n",
       "   11.0,\n",
       "   6.0,\n",
       "   16.0,\n",
       "   6.0,\n",
       "   11.0,\n",
       "   0.0,\n",
       "   6.0,\n",
       "   -1.0,\n",
       "   -5.0,\n",
       "   -5.0,\n",
       "   -4.0,\n",
       "   -6.0,\n",
       "   -2.0,\n",
       "   -4.0,\n",
       "   5.0,\n",
       "   -4.0,\n",
       "   11.0]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.7014085161614947,\n",
       "  'mean_inference_ms': 0.5443387050658153,\n",
       "  'mean_action_processing_ms': 0.09098077690653679,\n",
       "  'mean_env_wait_ms': 0.450588418440034,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'num_faulty_episodes': 0,\n",
       " 'num_healthy_workers': 0,\n",
       " 'num_agent_steps_sampled': 637,\n",
       " 'num_agent_steps_trained': 637,\n",
       " 'num_env_steps_sampled': 640,\n",
       " 'num_env_steps_trained': 640,\n",
       " 'num_env_steps_sampled_this_iter': 128,\n",
       " 'num_env_steps_trained_this_iter': 128,\n",
       " 'timesteps_total': 640,\n",
       " 'num_steps_trained_this_iter': 128,\n",
       " 'agent_timesteps_total': 637,\n",
       " 'timers': {'training_iteration_time_ms': 374.85,\n",
       "  'learn_time_ms': 149.948,\n",
       "  'learn_throughput': 853.629},\n",
       " 'counters': {'num_env_steps_sampled': 640,\n",
       "  'num_env_steps_trained': 640,\n",
       "  'num_agent_steps_sampled': 637,\n",
       "  'num_agent_steps_trained': 637},\n",
       " 'done': False,\n",
       " 'episodes_total': 39,\n",
       " 'training_iteration': 5,\n",
       " 'trial_id': 'default',\n",
       " 'experiment_id': '2ca3d8dbf2f943809a7328e48b51cb92',\n",
       " 'date': '2022-11-07_17-42-52',\n",
       " 'timestamp': 1667842972,\n",
       " 'time_this_iter_s': 0.2838468551635742,\n",
       " 'time_total_s': 1.8772540092468262,\n",
       " 'pid': 26711,\n",
       " 'hostname': 'ip-192-168-0-7.eu-west-2.compute.internal',\n",
       " 'node_ip': '127.0.0.1',\n",
       " 'config': {'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'placement_strategy': 'PACK',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'env': 'poker',\n",
       "  'env_config': {},\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'clip_rewards': None,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'disable_env_checking': False,\n",
       "  'num_workers': 0,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'sample_async': False,\n",
       "  'enable_connectors': False,\n",
       "  'rollout_fragment_length': 128,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'validate_workers_after_construction': True,\n",
       "  'ignore_worker_failures': False,\n",
       "  'recreate_failed_workers': False,\n",
       "  'restart_failed_sub_environments': False,\n",
       "  'num_consecutive_worker_failures_tolerance': 100,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'compress_observations': False,\n",
       "  'enable_tf1_exec_eagerly': False,\n",
       "  'sampler_perf_stats_ema_coef': None,\n",
       "  'gamma': 0.99,\n",
       "  'lr': 5e-05,\n",
       "  'train_batch_size': 128,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': False,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_config': {},\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_sample_timeout_s': 180.0,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'evaluation_config': {'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'placement_strategy': 'PACK',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'env': 'poker',\n",
       "   'env_config': {},\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'clip_rewards': None,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'disable_env_checking': False,\n",
       "   'num_workers': 0,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'sample_async': False,\n",
       "   'enable_connectors': False,\n",
       "   'rollout_fragment_length': 128,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'validate_workers_after_construction': True,\n",
       "   'ignore_worker_failures': False,\n",
       "   'recreate_failed_workers': False,\n",
       "   'restart_failed_sub_environments': False,\n",
       "   'num_consecutive_worker_failures_tolerance': 100,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'compress_observations': False,\n",
       "   'enable_tf1_exec_eagerly': False,\n",
       "   'sampler_perf_stats_ema_coef': None,\n",
       "   'gamma': 0.99,\n",
       "   'lr': 5e-05,\n",
       "   'train_batch_size': 128,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    '_disable_action_flattening': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': False,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'StochasticSampling'},\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_config': {},\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_sample_timeout_s': 180.0,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'evaluation_config': {'env': 'poker'},\n",
       "   'off_policy_estimation_methods': {},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'enable_async_evaluation': False,\n",
       "   'in_evaluation': False,\n",
       "   'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "   'keep_per_episode_custom_metrics': False,\n",
       "   'metrics_episode_collection_timeout_s': 60.0,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_iteration': None,\n",
       "   'min_train_timesteps_per_iteration': 0,\n",
       "   'min_sample_timesteps_per_iteration': 0,\n",
       "   'logger_creator': None,\n",
       "   'logger_config': None,\n",
       "   'log_level': 'WARN',\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'seed': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': True,\n",
       "   'simple_optimizer': True,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': -1,\n",
       "   'min_iter_time_s': -1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'buffer_size': -1,\n",
       "   'prioritized_replay': -1,\n",
       "   'learning_starts': -1,\n",
       "   'replay_batch_size': -1,\n",
       "   'replay_sequence_length': None,\n",
       "   'prioritized_replay_alpha': -1,\n",
       "   'prioritized_replay_beta': -1,\n",
       "   'prioritized_replay_eps': -1,\n",
       "   'min_time_s_per_reporting': -1,\n",
       "   'min_train_timesteps_per_reporting': -1,\n",
       "   'min_sample_timesteps_per_reporting': -1,\n",
       "   'input_evaluation': -1,\n",
       "   'lr_schedule': None,\n",
       "   'use_critic': True,\n",
       "   'use_gae': True,\n",
       "   'kl_coeff': 0.2,\n",
       "   'sgd_minibatch_size': 128,\n",
       "   'num_sgd_iter': 30,\n",
       "   'shuffle_sequences': True,\n",
       "   'vf_loss_coeff': 1.0,\n",
       "   'entropy_coeff': 0.0,\n",
       "   'entropy_coeff_schedule': None,\n",
       "   'clip_param': 0.3,\n",
       "   'vf_clip_param': 10.0,\n",
       "   'grad_clip': None,\n",
       "   'kl_target': 0.01,\n",
       "   'vf_share_layers': -1,\n",
       "   'lambda': 1.0,\n",
       "   'input': 'sampler',\n",
       "   'multiagent': {'policies': {'random': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f26a30>,\n",
       "     'Heuristic_10': <ray.rllib.policy.policy.PolicySpec at 0x7fd9555c6eb0>,\n",
       "     'Heuristic_100': <ray.rllib.policy.policy.PolicySpec at 0x7fd9555ca400>,\n",
       "     'Heuristic_1000': <ray.rllib.policy.policy.PolicySpec at 0x7fd9555ccfa0>,\n",
       "     'learned': <ray.rllib.policy.policy.PolicySpec at 0x7fd9555ccb20>},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': <function __main__.select_policy(agent_id, episode, **kwargs)>,\n",
       "    'policies_to_train': ['learned'],\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "   'create_env_on_driver': False,\n",
       "   'custom_eval_function': None,\n",
       "   'framework': 'tf',\n",
       "   'num_cpus_for_driver': 1},\n",
       "  'off_policy_estimation_methods': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'enable_async_evaluation': False,\n",
       "  'in_evaluation': False,\n",
       "  'sync_filters_on_rollout_workers_timeout_s': 60.0,\n",
       "  'keep_per_episode_custom_metrics': False,\n",
       "  'metrics_episode_collection_timeout_s': 60.0,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_iteration': None,\n",
       "  'min_train_timesteps_per_iteration': 0,\n",
       "  'min_sample_timesteps_per_iteration': 0,\n",
       "  'logger_creator': None,\n",
       "  'logger_config': None,\n",
       "  'log_level': 'WARN',\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'seed': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': True,\n",
       "  'simple_optimizer': True,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': -1,\n",
       "  'min_iter_time_s': -1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'buffer_size': -1,\n",
       "  'prioritized_replay': -1,\n",
       "  'learning_starts': -1,\n",
       "  'replay_batch_size': -1,\n",
       "  'replay_sequence_length': None,\n",
       "  'prioritized_replay_alpha': -1,\n",
       "  'prioritized_replay_beta': -1,\n",
       "  'prioritized_replay_eps': -1,\n",
       "  'min_time_s_per_reporting': -1,\n",
       "  'min_train_timesteps_per_reporting': -1,\n",
       "  'min_sample_timesteps_per_reporting': -1,\n",
       "  'input_evaluation': -1,\n",
       "  'lr_schedule': None,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'num_sgd_iter': 30,\n",
       "  'shuffle_sequences': True,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'vf_share_layers': -1,\n",
       "  'lambda': 1.0,\n",
       "  'input': 'sampler',\n",
       "  'multiagent': {'policies': {'random': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f560a0>,\n",
       "    'Heuristic_10': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f56130>,\n",
       "    'Heuristic_100': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f563a0>,\n",
       "    'Heuristic_1000': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f56430>,\n",
       "    'learned': <ray.rllib.policy.policy.PolicySpec at 0x7fd954f564c0>},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': <function __main__.select_policy(agent_id, episode, **kwargs)>,\n",
       "   'policies_to_train': ['learned'],\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'callbacks': ray.rllib.algorithms.callbacks.DefaultCallbacks,\n",
       "  'create_env_on_driver': False,\n",
       "  'custom_eval_function': None,\n",
       "  'framework': 'tf',\n",
       "  'num_cpus_for_driver': 1},\n",
       " 'time_since_restore': 1.8772540092468262,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 5,\n",
       " 'warmup_time': 11.530540943145752,\n",
       " 'perf': {'cpu_util_percent': 23.65, 'ram_util_percent': 63.849999999999994}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a = np.zeros(16)\n",
    "a = np.concatenate([a, np.zeros(8)])\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('stocktake')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03af9a79c2dca760cc1a7eb3ed3e88bcac6dcfea843cc60b832e817055251c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
