{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Training Poker Using RLLib*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from poker_env import PokerEnv\n",
    "from agents.random_policy import RandomActions\n",
    "from agents.heuristic_policy import HeuristicPolicy\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.a3c import A3C\n",
    "from ray.rllib.algorithms.sac import SAC\n",
    "from ray.rllib.algorithms.dqn import DQN\n",
    "from gym import spaces\n",
    "import mpu\n",
    "import numpy as np\n",
    "import ray\n",
    "from ray.rllib.models import MODEL_DEFAULTS\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Rllib, a policy function needs to be passed to map agent IDs to the policy to use. We also create and register the learning environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_policy(agent_id, episode, **kwargs):\n",
    "    if agent_id == 0:\n",
    "        return \"a3c\"\n",
    "    elif agent_id == 1:\n",
    "        return \"sac\"\n",
    "    elif agent_id == 2:\n",
    "        return \"dqn\"\n",
    "    elif agent_id == 3:\n",
    "        return \"ppo\"\n",
    "    return \"learned4\"\n",
    "\n",
    "def env_creator(config):\n",
    "    env = PokerEnv(select_policy, config)\n",
    "    return env\n",
    "\n",
    "register_env(\"poker\", lambda config: env_creator(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The config describes all aspects of the training. A full list of the parameters is found here: https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py \n",
    "\n",
    "In this example, we have used a default config that runs the PPO algorithm. Other heuristic agents have been defined that will play the game with the single learning agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:41:13,613\tWARNING ppo.py:351 -- `train_batch_size` (4000) cannot be achieved with your other settings (num_workers=8 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 500.\n",
      "2022-11-10 14:41:17,178\tINFO worker.py:1528 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76848)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76848)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76845)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76845)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76844)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76844)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76849)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76849)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76850)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76850)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76846)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76846)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76847)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76847)\u001b[0m   from collections import Iterable\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76851)\u001b[0m /Users/aaronedwards/Documents/Projects/RL_Poker_Env/poker_env.py:10: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=76851)\u001b[0m   from collections import Iterable\n",
      "2022-11-10 14:41:35,005\tINFO trainable.py:164 -- Trainable.setup took 21.401 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-11-10 14:41:35,031\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "heuristic_observation_space = spaces.Dict({\n",
    "            \"hand\": spaces.Box(0, 1, shape=(24, )),\n",
    "            \"community\": spaces.Box(0, 1, shape=(24, ))\n",
    "        })\n",
    "action_space = spaces.Discrete(3)\n",
    "\n",
    "#Defines the learning models architecture. \n",
    "model = MODEL_DEFAULTS.update({'fcnet_hiddens': [512, 512], 'fcnet_activation': 'relu'})\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    #Each rollout worker uses a single cpu\n",
    "    .rollouts(num_rollout_workers=8, num_envs_per_worker=1)\\\n",
    "    .training(train_batch_size=4000, gamma=0.99, model=model, lr=0.0004)\\\n",
    "    .environment(disable_env_checking=True)\\\n",
    "    .multi_agent(\n",
    "        policies={\n",
    "            #These policies thave pre-definded polices that dont learn.\n",
    "            \"a3c\": PolicySpec(config=A3C.get_default_config()),\n",
    "            \"sac\": PolicySpec(config=SAC.get_default_config()),\n",
    "            \"dqn\": PolicySpec(config=DQN.get_default_config()),\n",
    "            #Passing nothing causes this agent to deafult to using a PPO policy\n",
    "            \"ppo\": PolicySpec(\n",
    "                config={}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn=select_policy,\n",
    "        policies_to_train=['a3c', 'sac', 'dqn', 'ppo'],\n",
    "    )\\\n",
    "    .resources(num_gpus=0)\\\n",
    "    .framework('torch')\n",
    ")\n",
    "trainer = config.build(env=\"poker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start up tensorboard\n",
    "#!tensorboard --logdir=~/ray_results --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop, each run will rollout x timesteps (where x is train_batch_size). An weight update is then applied using the rollout data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-10 14:41:37,708\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves a checkpoint of the trainer.\n",
    "trainer.save(\"checkpoint/ppo_poker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "03af9a79c2dca760cc1a7eb3ed3e88bcac6dcfea843cc60b832e817055251c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
